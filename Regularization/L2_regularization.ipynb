{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "import scipy.io\n",
    "from scipy.special import xlogy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    train_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:])\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:])\n",
    "\n",
    "    test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:])\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:])\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:])\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(Z):\n",
    "    return np.tanh(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    return np.maximum(Z, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(Z, alpha = 0.01):\n",
    "    np.where(Z > 0, Z, Z * alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dif_sigmoid(Z):\n",
    "    return (1-np.power(Z, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dif_tanh(Z):\n",
    "    return 1-(tanh(Z)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dif_relu(Z):\n",
    "    return (Z>0).astype(Z.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dif_leaky_relu(Z, alpha = 0.01):\n",
    "    dz = np.ones_like(Z)\n",
    "    dz[Z < 0] = alpha\n",
    "    return dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_fun(name):\n",
    "    if name=='relu':\n",
    "        return relu\n",
    "    elif name=='sigmoid':\n",
    "        return sigmoid\n",
    "    elif name=='leaky_relu':\n",
    "        return leaky_relu\n",
    "    elif name=='tanh':\n",
    "        return tanh\n",
    "    else:\n",
    "        return tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_activation(name):\n",
    "    if name=='relu':\n",
    "        return dif_relu\n",
    "    elif name=='sigmoid':\n",
    "        return dif_sigmoid\n",
    "    elif name=='leaky_relu':\n",
    "        return dif_leaky_relu\n",
    "    elif name=='tanh':\n",
    "        return dif_tanh\n",
    "    else:\n",
    "        return dif_tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_x_orig shape (209, 64, 64, 3)\n",
      "train_set_y_orig (209,)\n",
      "test_set_x_orig (50, 64, 64, 3)\n",
      "test_set_y_orig (50,)\n",
      "classes (2,)\n"
     ]
    }
   ],
   "source": [
    "train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes = load_dataset()\n",
    "print('train_set_x_orig shape', train_set_x_orig.shape)\n",
    "print('train_set_y_orig',train_set_y_orig.shape)\n",
    "print(\"test_set_x_orig\",test_set_x_orig.shape)\n",
    "print(\"test_set_y_orig\",test_set_y_orig.shape)\n",
    "print('classes',classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig):\n",
    "    train_x = train_set_x_orig.reshape(train_set_x_orig.shape[0], train_set_x_orig.shape[1]*train_set_x_orig.shape[2]*train_set_x_orig.shape[3])/255.\n",
    "    test_x = test_set_x_orig.reshape(test_set_x_orig.shape[0],test_set_x_orig.shape[1]*test_set_x_orig.shape[2]*test_set_x_orig.shape[3])/255.\n",
    "    train_y = train_set_y_orig.reshape(-1,1)\n",
    "    test_y = test_set_y_orig.reshape(-1,1)\n",
    "    print('train_x shape', train_x.shape)\n",
    "    print('train_y',train_y.shape)\n",
    "    print(\"test_x\",test_x.shape)\n",
    "    print(\"test_y\",test_y.shape)\n",
    "    return train_x,test_x,train_y,test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_weights(X,Y,h_nodes,hidden_layer):\n",
    "    np.random.seed(42) \n",
    "    x_nodes = X.shape[1]\n",
    "    y_nodes = Y.shape[1]\n",
    "\n",
    "    w = {}\n",
    "    b = {} \n",
    "    \n",
    "    for i in range(len(h_nodes)):\n",
    "        if i==0:\n",
    "            n_l_1 = x_nodes\n",
    "        else:\n",
    "            n_l_1 = h_nodes[i-1]\n",
    "        \n",
    "        w[i] = np.random.randn(n_l_1 ,h_nodes[i])*np.sqrt(2/n_l_1)\n",
    "        b[i] = np.random.randn(1,h_nodes[i]) * np.sqrt(2/n_l_1)\n",
    "        \n",
    "#         for k,v in w.items():\n",
    "#             print('Key value ', k ,v.shape)\n",
    "              \n",
    "#     w[0] = np.random.randn(x_nodes ,h_nodes)\n",
    "#     b[0] = np.zeros([1,h_nodes])\n",
    "    \n",
    "#     for i in range(1,hidden_layer):\n",
    "#         w[i] = np.random.randn(h_nodes[i-1] ,h_nodes[i])\n",
    "#         b[i] = np.zeros([1,h_nodes[i]])  \n",
    "#     w[i] = np.random.randn(h_nodes[i] ,y_nodes)\n",
    "#     b[i] = np.zeros([1,y_nodes])\n",
    "\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagate(X,w,b, activation,hidden_layer,h_nodes):\n",
    "    m = X.shape[0]\n",
    "    Z = {}\n",
    "    A = {}    \n",
    "    for l in range(len(h_nodes)):\n",
    "#         print(l)\n",
    "        if l == 0:\n",
    "            input_X = X\n",
    "        else:\n",
    "            input_X = A[l-1]\n",
    "        Z[l] = (np.dot(input_X,w[l])+b[l])\n",
    "        A[l] = activation_fun(activation[l])(Z[l])\n",
    "    \n",
    "    return Z, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costfunction(Y,A,w,lambd):   \n",
    "    m = Y.shape[0]\n",
    "    last_index = len(A)-1\n",
    "    for k,v in w.items():\n",
    "        sum_w = np.sum(np.square(k))\n",
    "    regularized_cost = lambd*np.sum(sum_w)/(2*m)\n",
    "    cost = np.nansum(-1/m*np.sum(Y*np.log(A[last_index]) + (1-Y)*np.log(1-A[last_index]))) + regularized_cost\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def back_prpagate(X,Y,Z,A,w,b,activation, hidden_layer,h_nodes):\n",
    "\n",
    "#     m = X.shape[0]\n",
    "#     L= hidden_layer \n",
    "# #     dz2 = (A2-Y)\n",
    "# #     dw2 = 1/m*(np.dot(A1.T,dz2))\n",
    "# #     db2 = 1/m*(np.sum(dz2, axis=0, keepdims=True))\n",
    "    \n",
    "# #     dz1 = np.dot(dz2, w2.T) * activation_fun(activation)(A1)\n",
    "# #     dw1 = 1/m*(np.dot(X.T,dz1))\n",
    "# #     db1 = 1/m*(np.sum(dz1, axis=0, keepdims=True))    \n",
    "#     dz = {}\n",
    "#     da = {}\n",
    "#     dw = {}\n",
    "#     db = {}\n",
    "#     for l in range(len(h_nodes)-1, -1, -1):\n",
    "# #         print('A shape', len(A))\n",
    "#         if l==len(h_nodes)-1:\n",
    "#             dz[l] = (A[l] - Y)\n",
    "#             dw[l] = (1./m * np.dot(A[l-1].T, dz[l]))\n",
    "#             db[l] = (1./m * np.sum(dz[l]))\n",
    "\n",
    "#         else:\n",
    "#             dz[l] = ((np.dot(dz[l+1], w[l+1].T)) * derivative_activation(activation[l])(Z[l]))\n",
    "#             if l!=0:\n",
    "#                 input_X = A[l-1]\n",
    "#             else:\n",
    "#                 input_X = X\n",
    "#             dw[l] = (1./m * np.dot(input_X.T, dz[l]))\n",
    "#             db[l] = (1./m * np.sum(dz[l]))\n",
    "                \n",
    "#     return dz,dw,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BACKWARD PROPAGATION (TO FIND GRADIENT)\n",
    "def back_prpagate(X,Y,Z,A,w,b,activation, hidden_layer,h_nodes):\n",
    "    \"\"\"Performs backward propagation and calculates derivative value for a layer\n",
    "\n",
    "    Arguments:\n",
    "    X -- array_like Data\n",
    "    Y -- array_like True labels\n",
    "    A -- predicted output, dict\n",
    "    Z -- intermidiate dot product , dict\n",
    "    w -- dict of weights\n",
    "    b -- dict of bias\n",
    "    activationion -- list of actiations used at particular hidden layer\n",
    "    hidden_layer -- number of hidden layers, integer\n",
    "    h_nodes -- number of hidden nodes in each hidden laayer, list    \n",
    "\n",
    "    Returns:\n",
    "    dw -- derivative of weight, dict\n",
    "    db -- derivative of bias,dict\n",
    "    dz -- cache,dict\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[0]\n",
    "    L= hidden_layer \n",
    "#     dz2 = (A2-Y)\n",
    "#     dw2 = 1/m*(np.dot(A1.T,dz2))\n",
    "#     db2 = 1/m*(np.sum(dz2, axis=0, keepdims=True))\n",
    "    \n",
    "    \n",
    "#     dz1 = np.dot(dz2, w2.T) * activation_fun(activation)(A1)\n",
    "#     dw1 = 1/m*(np.dot(X.T,dz1))\n",
    "#     db1 = 1/m*(np.sum(dz1, axis=0, keepdims=True))    \n",
    "    dz = {}\n",
    "    da = {}\n",
    "    dw = {}\n",
    "    db = {}\n",
    "    for l in range(len(h_nodes)-1, -1, -1):\n",
    "#         print('A shape', len(A))\n",
    "        if l==len(h_nodes)-1:\n",
    "            dz[l] = (A[l] - Y)\n",
    "            dw[l] = (1./m * np.dot(A[l-1].T, dz[l]))\n",
    "            db[l] = (1./m * np.sum(dz[l]))\n",
    "            da[l-1] = np.dot(dz[l], w[l].T)\n",
    "\n",
    "        else:\n",
    "            dz[l] = (np.multiply(np.int64(A[l]>0), da[l]) * derivative_activation(activation[l])(Z[l]))\n",
    "            if l!=0:\n",
    "                input_X = A[l-1]\n",
    "            else:\n",
    "                input_X = X\n",
    "            dw[l] = (1./m * np.dot(input_X.T, dz[l]))\n",
    "            db[l] = (1./m * np.sum(dz[l]))\n",
    "            da[l-1] = np.dot(dz[l], w[l].T)\n",
    "    return dz,dw,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(X,w,b,dw,db,learning_rate,hidden_layer,h_nodes,lambd):\n",
    "    L=hidden_layer\n",
    "    m = X.shape[0]\n",
    "    for i in range(len(h_nodes)):\n",
    "        w[i] = w[i] - learning_rate*(dw[i]+lambd*w[i]/m)\n",
    "        b[i] = b[i] - learning_rate*db[i]\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X,Y,test_x,test_y,w,b, learning_rate,num_iterations,activation,hidden_layer,h_nodes,lambd):\n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        Z,A = forward_propagate(X,w,b,activation,hidden_layer,h_nodes)\n",
    "        cost = costfunction(Y,A,w,lambd)\n",
    "#         print('cost', cost)\n",
    "        dz,dw,db = back_prpagate(X,Y,Z,A,w,b,activation,hidden_layer,h_nodes)\n",
    "        w,b = update_weights(X,w,b,dw,db,learning_rate,hidden_layer,h_nodes,lambd)\n",
    "        \n",
    "        if i%(num_iterations/10) == 0:\n",
    "#             print(cost)\n",
    "            _,A_train = forward_propagate(X,w,b, activation,hidden_layer,h_nodes)\n",
    "            A_train1 = np.where(A_train[len(A_train)-1]>0.5 , 1, 0)\n",
    "        \n",
    "            _,A_test = forward_propagate(test_x,w,b,activation,hidden_layer,h_nodes)\n",
    "            A_test1 = np.where(A_test[len(A_test)-1]>0.5 , 1, 0)\n",
    "            acc_train = accuracy_score(Y, A_train1)\n",
    "            acc_test = accuracy_score(test_y , A_test1)\n",
    "        \n",
    "            print('Iteration: ', i, end = '')\n",
    "            print('\\tLoss {:.4f}\\t'.format(cost), end = '')\n",
    "            print('\\tTraining Accuracy: {:.4f}\\t'.format(acc_train),end = '')\n",
    "            print('Testing Accuracy: {:.4f}'.format(acc_test))\n",
    "    return w,b,dw,db, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes, num_iterations, learning_rate,lambd):\n",
    "    train_x,test_x,train_y,test_y = preprocess(train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig)\n",
    "    hidden_layer = 4\n",
    "    h_nodes = [20,16,8,4,train_y.shape[1]]\n",
    "    activation = ['relu','relu','relu','relu','sigmoid']\n",
    "    w,b = initial_weights(train_x,train_y,h_nodes,hidden_layer)\n",
    "    w,b,dw,db,cost = train_model(train_x,train_y,test_x,test_y,w,b,learning_rate=learning_rate,num_iterations=num_iterations,\n",
    "                                 activation = activation, hidden_layer = hidden_layer,h_nodes = h_nodes,lambd = lambd )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x shape (209, 12288)\n",
      "train_y (209, 1)\n",
      "test_x (50, 12288)\n",
      "test_y (50, 1)\n",
      "Iteration:  0\tLoss 0.6709\t\tTraining Accuracy: 0.6555\tTesting Accuracy: 0.3400\n",
      "Iteration:  50\tLoss 0.6487\t\tTraining Accuracy: 0.6555\tTesting Accuracy: 0.3400\n",
      "Iteration:  100\tLoss 0.6336\t\tTraining Accuracy: 0.6555\tTesting Accuracy: 0.3400\n",
      "Iteration:  150\tLoss 0.6148\t\tTraining Accuracy: 0.6555\tTesting Accuracy: 0.3400\n",
      "Iteration:  200\tLoss 0.5948\t\tTraining Accuracy: 0.6986\tTesting Accuracy: 0.4800\n",
      "Iteration:  250\tLoss 0.5535\t\tTraining Accuracy: 0.8038\tTesting Accuracy: 0.6200\n",
      "Iteration:  300\tLoss 0.5269\t\tTraining Accuracy: 0.8421\tTesting Accuracy: 0.6800\n",
      "Iteration:  350\tLoss 0.4946\t\tTraining Accuracy: 0.8325\tTesting Accuracy: 0.7600\n",
      "Iteration:  400\tLoss 0.4601\t\tTraining Accuracy: 0.8469\tTesting Accuracy: 0.7400\n",
      "Iteration:  450\tLoss 0.4332\t\tTraining Accuracy: 0.8612\tTesting Accuracy: 0.7200\n"
     ]
    }
   ],
   "source": [
    "train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes = load_dataset()\n",
    "# num_iterations = int(input(\"Enter iterations : \"))\n",
    "# learning_rate = float(input(\"Enter learning rate : \"))\n",
    "num_iterations = 500\n",
    "learning_rate = 0.005\n",
    "lambd = 0.7\n",
    "model(train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes, num_iterations, learning_rate,lambd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
