{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shallow Neural Network (1 hidden layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data (cat/non-cat)\n",
    "def load_dataset():\n",
    "    \"\"\"Loads the Cat vs Non-Cat dataset\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_train, y_train, X_test, y_test, classes: Arrays\n",
    "    Dataset splitted into train and test with classes\n",
    "    \"\"\"\n",
    "    train_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:])\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:])\n",
    "\n",
    "    test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:])\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:])\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:])\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating sigmoid\n",
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating tanh\n",
    "def tanh(Z):\n",
    "    return np.tanh(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating relu\n",
    "def relu(Z):\n",
    "    return np.maximum(Z, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating leaky-relu\n",
    "def leaky_relu(Z, alpha = 0.01):\n",
    "    np.where(Z > 0, Z, Z * alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating first derivative of sigmoid\n",
    "def dif_sigmoid(Z):\n",
    "    return (1-np.power(Z, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating first derivative of tanh\n",
    "def dif_tanh(Z):\n",
    "    return 1-(tanh(Z)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating first derivative of relu\n",
    "def dif_relu(Z):\n",
    "    return (Z>0).astype(Z.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating first derivative of leaky relu\n",
    "def dif_leaky_relu(Z, alpha = 0.01):\n",
    "    dz = np.ones_like(Z)\n",
    "    dz[Z < 0] = alpha\n",
    "    return dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get activation function\n",
    "def activation_fun(name):\n",
    "    if name=='relu':\n",
    "        return relu\n",
    "    elif name=='sigmoid':\n",
    "        return sigmoid\n",
    "    elif name=='leaky_relu':\n",
    "        return leaky_relu\n",
    "    elif name=='tanh':\n",
    "        return tanh\n",
    "    else:\n",
    "        return tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get first derivative of activation function\n",
    "def derivative_activation(name):\n",
    "    if name=='relu':\n",
    "        return dif_relu\n",
    "    elif name=='sigmoid':\n",
    "        return dif_sigmoid\n",
    "    elif name=='leaky_relu':\n",
    "        return dif_leaky_relu\n",
    "    elif name=='tanh':\n",
    "        return dif_tanh\n",
    "    else:\n",
    "        return dif_tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_x_orig shape (209, 64, 64, 3)\n",
      "train_set_y_orig (209,)\n",
      "test_set_x_orig (50, 64, 64, 3)\n",
      "test_set_y_orig (50,)\n",
      "classes (2,)\n"
     ]
    }
   ],
   "source": [
    "# Printing the shape of the training and testing data\n",
    "train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes = load_dataset()\n",
    "print('train_set_x_orig shape', train_set_x_orig.shape)\n",
    "print('train_set_y_orig',train_set_y_orig.shape)\n",
    "print(\"test_set_x_orig\",test_set_x_orig.shape)\n",
    "print(\"test_set_y_orig\",test_set_y_orig.shape)\n",
    "print('classes',classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the training and test examples\n",
    "def preprocess(train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig):\n",
    "    train_x = train_set_x_orig.reshape(train_set_x_orig.shape[0], train_set_x_orig.shape[1]*train_set_x_orig.shape[2]*train_set_x_orig.shape[3])/255.\n",
    "    test_x = test_set_x_orig.reshape(test_set_x_orig.shape[0],test_set_x_orig.shape[1]*test_set_x_orig.shape[2]*test_set_x_orig.shape[3])/255.\n",
    "    train_y = train_set_y_orig.reshape(-1,1)\n",
    "    test_y = test_set_y_orig.reshape(-1,1)\n",
    "    print('train_x shape', train_x.shape)\n",
    "    print('train_y',train_y.shape)\n",
    "    print(\"test_x\",test_x.shape)\n",
    "    print(\"test_y\",test_y.shape)\n",
    "    return train_x,test_x,train_y,test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining initail weights to w1,b1 and w2,b2\n",
    "def initial_weights(X,hidden_nodes):\n",
    "    \"\"\"\n",
    "    This function creates a vector of zeros of shape (X.shape[1], 1) for w and initializes b to 0.\n",
    "    \n",
    "    Argument:\n",
    "    X -- training dataset\n",
    "    hidden_nodes -- number of hidden nodes , integer\n",
    "    \n",
    "    Returns:\n",
    "    w1 -- initialized vector of shape (X.shape[1], hidden_nodes)\n",
    "    b1 -- initialized scalar (corresponds to the bias)\n",
    "    w2 -- initialized vector of shape (hidden_nodes, 1)\n",
    "    b2 -- initialized scalar (corresponds to the bias)\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(2) # we set up a seed so that your output matches ours although the initialization is random.\n",
    "    w1 = np.random.randn(X.shape[1] ,hidden_nodes)\n",
    "    b1 = np.zeros([1,hidden_nodes])\n",
    "    w2 = np.random.randn(hidden_nodes ,1)\n",
    "    b2 = np.zeros([1,1])\n",
    "    return w1,b1,w2,b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FORWARD PROPAGATION\n",
    "def forward_propagate(X,w1,b1,w2,b2, activation='relu'):\n",
    "    \"\"\"\n",
    "    This functions performs forward propagation and calculates output value\n",
    "    \n",
    "    Argument:\n",
    "    X -- training dataset\n",
    "    w1 -- weight between input and hidden layer\n",
    "    b1 -- bias between input and hidden layer\n",
    "    w2 -- weight between hidden and output layer\n",
    "    b2 -- bias between hidden and output layer\n",
    "    \n",
    "    Returns:\n",
    "    A1 -- yhat for the training data\n",
    "    Z1 -- Dot product between X and w\n",
    "    A2 -- yhat(predicted output) for the training data\n",
    "    Z2 -- Dot product between A1, w\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    Z1 = (np.dot(X,w1)+b1)\n",
    "    A1 = activation_fun(activation)(Z1)\n",
    "    Z2 = (np.dot(A1,w2) +b2)\n",
    "    A2 = activation_fun('sigmoid')(Z2)\n",
    "    \n",
    "    return Z1,Z2,A1,A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating loss using the cost function\n",
    "def costfunction(Y,A2):\n",
    "    \"\"\"\n",
    "    This function calculates the loss between the predicted and actual output\n",
    "    \n",
    "    Argument:\n",
    "    Y -- actual output\n",
    "    A2 -- predicted output\n",
    "    \n",
    "    Returns:\n",
    "    cost -- loss between the predicted and actual output\n",
    "    \"\"\"\n",
    "    m = Y.shape[0]\n",
    "    cost = -1/m*np.sum(Y*np.log(A2) + (1-Y)*np.log(1-A2))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BACKWARD PROPAGATION (TO FIND GRADIENT)\n",
    "def back_prpagate(X,Y,Z1,Z2,A1,A2,w1,b1,w2,b2,activation):\n",
    "    \"\"\"Performs backward propagation and calculates derivative value for a layer\n",
    "\n",
    "    Arguments:\n",
    "    X -- array_like Data\n",
    "    Y -- array_like True labels\n",
    "    A -- predicted output\n",
    "\n",
    "    Returns:\n",
    "    dw -- derivative of weight\n",
    "    db -- derivative of bias\n",
    "    dz -- cache\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "               \n",
    "    dz2 = (A2-Y)\n",
    "    dw2 = 1/m*(np.dot(A1.T,dz2))\n",
    "    db2 = 1/m*(np.sum(dz2, axis=0, keepdims=True))\n",
    "    \n",
    "    dz1 = np.dot(dz2, w2.T) * activation_fun(activation)(A1)\n",
    "    dw1 = 1/m*(np.dot(X.T,dz1))\n",
    "    db1 = 1/m*(np.sum(dz1, axis=0, keepdims=True))\n",
    "    \n",
    "    return dz2,dw2,db2,dz1,dw1,db1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(w1, b1,w2,b2,dw1,db1,dw2,db2,learning_rate):\n",
    "    \"\"\"\n",
    "    This function updates the weight and bias\n",
    "    \n",
    "    Argument:\n",
    "    w -- weight\n",
    "    b -- bias\n",
    "    dw -- derivative of weight\n",
    "    db -- derivative of bias\n",
    "    \n",
    "    Returns:\n",
    "    w1 -- weight between input and hidden layer\n",
    "    b1 -- bias between input and hidden layer\n",
    "    w2 -- weight between hidden and output layer\n",
    "    b2 -- bias between hidden and output layer\n",
    "    \"\"\"\n",
    "    w1 = w1-learning_rate*dw1\n",
    "    b1 = b1-learning_rate*db1\n",
    "    \n",
    "    w2 = w2-learning_rate*dw2\n",
    "    b2 = b2-learning_rate*db2\n",
    "    return w1,b1,w2,b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X,Y,test_x,test_y,w1,b1,w2,b2, learning_rate = 0.01, num_iterations = 100,activation = 'relu'):\n",
    "    \"\"\"\n",
    "    This function  trains the model with the number of iterations\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weight\n",
    "    b -- bias, a scalar\n",
    "    X -- training data \n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    \n",
    "    Returns:\n",
    "    w -- weight\n",
    "    b -- bias\n",
    "    dw -- derivative of weight\n",
    "    db -- derivative of bias\n",
    "    cost -- loss     \n",
    "    \"\"\"\n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        Z1,Z2,A1,A2 = forward_propagate(X,w1,b1,w2,b2,activation)\n",
    "        cost = costfunction(Y,A2)\n",
    "        dz2,dw2,db2,dz1,dw1,db1 = back_prpagate(X,Y,Z1,Z2,A1,A2,w1,b1,w2,b2,activation)\n",
    "        w1,b1,w2,b2 = update_weights(w1, b1,w2,b2,dw1,db1,dw2,db2,learning_rate)\n",
    "        \n",
    "        if i%(num_iterations/10) == 0:\n",
    "#             print(cost)\n",
    "            _,_,_,A_train = forward_propagate(X,w1,b1,w2,b2)\n",
    "            A_train = np.where(A_train>0.5 , 1, 0)\n",
    "        \n",
    "            _,_,_,A_test = forward_propagate(test_x,w1,b1,w2,b2)\n",
    "            A_test = np.where(A_test>0.5 , 1, 0)\n",
    "            acc_train = accuracy_score(Y, A_train)\n",
    "            acc_test = accuracy_score(test_y , A_test)\n",
    "        \n",
    "            print('Iteration: ', i, end = '')\n",
    "            print('\\t\\tTraining Accuracy: {:.4f}\\t'.format(acc_train),end = '')\n",
    "            print('Testing Accuracy: {:.4f}'.format(acc_test))\n",
    "\n",
    "    return w1,b1,w2,b2,dw1,db1,dw2,db2, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes, num_iterations, learning_rate):\n",
    "    train_x,test_x,train_y,test_y = preprocess(train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig)\n",
    "    hidden_nodes = 100\n",
    "    activation = 'relu'\n",
    "    w1,b1,w2,b2 = initial_weights(train_x,hidden_nodes)\n",
    "    w1,b1,w2,b2,dw1,db1,dw2,d2, cost = train_model(train_x,train_y,test_x,test_y,w1,b1,w2,b2, learning_rate = learning_rate, num_iterations = num_iterations,activation = activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter iterations : 1000\n",
      "Enter learning rate : 0.01\n",
      "train_x shape (209, 12288)\n",
      "train_y (209, 1)\n",
      "test_x (50, 12288)\n",
      "test_y (50, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\computervisionenv\\lib\\site-packages\\ipykernel_launcher.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  \n",
      "C:\\Users\\user\\Anaconda3\\envs\\computervisionenv\\lib\\site-packages\\ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in multiply\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\t\tTraining Accuracy: 0.3445\tTesting Accuracy: 0.6600\n",
      "Iteration:  100\t\tTraining Accuracy: 0.8086\tTesting Accuracy: 0.6400\n",
      "Iteration:  200\t\tTraining Accuracy: 0.8278\tTesting Accuracy: 0.6800\n",
      "Iteration:  300\t\tTraining Accuracy: 0.8373\tTesting Accuracy: 0.6800\n",
      "Iteration:  400\t\tTraining Accuracy: 0.8469\tTesting Accuracy: 0.7200\n",
      "Iteration:  500\t\tTraining Accuracy: 0.8708\tTesting Accuracy: 0.7200\n",
      "Iteration:  600\t\tTraining Accuracy: 0.8995\tTesting Accuracy: 0.7200\n",
      "Iteration:  700\t\tTraining Accuracy: 0.9139\tTesting Accuracy: 0.7000\n",
      "Iteration:  800\t\tTraining Accuracy: 0.9378\tTesting Accuracy: 0.6800\n",
      "Iteration:  900\t\tTraining Accuracy: 0.9474\tTesting Accuracy: 0.6800\n"
     ]
    }
   ],
   "source": [
    "train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes = load_dataset()\n",
    "num_iterations = int(input(\"Enter iterations : \"))\n",
    "learning_rate = float(input(\"Enter learning rate : \"))\n",
    "\n",
    "model(train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes, num_iterations, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
